\subsection{Factor Analysis}
\label{sec:CA}

Principal Component Analysis (PCA) \cite{} has been the most widely used CA technique to define the shape and appearance models used in AAMs \cite{}. However, the AAM definition introduced in section \ref{sec:aam} is broad enough to allow the use of other CA techniques to define such models. 

In fact, several authors have proposed the use of other CA techniques \cite{DelaTorre2008, Hamsici2009,Gonazalez2007} to define such models. On the other hand, probabilistic versions of PCA have also been used to motivate probabilistic formulations of AAMs

In this section we briefly review both PCA and Probabilistic PCA (PPCA). Note that in Section \ref{sec:paam} we will borrow key concepts from PPCA and other Probabilistic CA techniques in order to motivate and extend previous probabilistic formulation of AAMs.

%----------

\subsubsection{Principal Component Analysis}
\label{sec:PCA}

PCA finds a set of orthonormal projection bases $\mathbf{U}$ so that the latent space $\mathbf{Y}$ is the projection of the mean-centered training samples $\mathbf{\bar{X}} = \left( \mathbf{x}_1 - \bar{\mathbf{x}}, \ldots, \mathbf{x}_N - \bar{\mathbf{x}} \right)$ onto $\mathbf{U}$ (i.e. $\mathbf{Y} = \mathbf{U}^T \mathbf{\bar{X}}$). 

The optimization problem is defined as follows

\begin{equation}
	\mathbf{U}_o = \underset{\mathbf{U}}{\mathrm{arg\,max\;}} \mathrm{Tr} \left( \mathbf{U}^T \mathbf{S} \mathbf{U} \right), \,\, \mbox{s.t.} \,\, \mathbf{U}^T\mathbf{U} = \mathbf{I}
\end{equation}

where $\mathbf{S} = \frac{1}{N} \sum_{n=1}^N (\mathbf{x}_n - \bar{\mathbf{x}})(\mathbf{x}_n - \bar{\mathbf{x}})^T = \frac{1}{N} \mathbf{\bar{X}} \mathbf{\bar{X}}^T$ is the total scatter matrix and $\mathbf{I}$ is the identity matrix. 

The optimal projection bases $\mathbf{U}_o \in \mathcal{R}^{F \times M}$ are recovered by keeping the $M$ eigenvectors $\mathbf{U} = (\mathbf{u}_1, \ldots ,\mathbf{u}_M)$ that correspond to the $M$ largest eigenvalues of $\mathbf{S}$ (in the following we will assume that the eigenvalues are stored in a diagonal matrix $\mathbf{\Sigma}= \textrm{diag} \left( \lambda_1, \ldots, \lambda_M \right)$).

%----------

\subsubsection{Probabilistic PCA}
\label{sec:PPCA}

Probabilistic versions of PCA (PPCA) were independently proposed in \cite{Moghaddam1997,Roweis1998,Tipping1999} \footnote{In particular the Maximum Likelihood (ML)  solutions where provided  in \cite{Moghaddam1997,Tipping1999}, while Expectation Maximization (EM) solutions where presented in \cite{Roweis1998, Tipping1999}}. In these works the following probabilistic generative model was defined:

\begin{equation}
	\begin{aligned}
		\mathbf{x}_i & = \mathbf{W}\mathbf{y}_i + \bar{\mathbf{x}} + \boldsymbol{\epsilon}
		\\
		\mathbf{y} & \sim \mathcal{N}(\mathbf{0},\boldsymbol{\mathbf{I}}) 
		\\
		\boldsymbol{\epsilon} & \sim \mathcal{N}(\mathbf{0},\sigma^2\mathbf{I})
	\end{aligned}
	\label{eq:ppca_model}
\end{equation}

where $\mathbf{W}$ is the matrix that relates the latent variables $\mathbf{y}$ with the observed sample $\mathbf{x}$ and $\boldsymbol{\epsilon}$ is the sample noise which is assumed to be an isotropic Gaussian. The motivation is that, when $N < F$, the latent variables will offer a more compact representation of the dependencies between the observations. Denoting the parameters as $\theta = \left\{ \mathbf{W}, \bar{\mathbf{x}}, \sigma^2 \right\}$, the posterior probability over the latent variables is given by

\begin{equation}
p(\mathbf{y} | \mathbf{x}, \theta) = \mathcal{N} \left( \mathbf{M}^{-1} \mathbf{W}^T (\mathbf{x} - \bar{\mathbf{x}}), \sigma^2 \mathbf{M}^{-1} \right))
\end{equation}

where $\mathbf{M} = \mathbf{W}^T\mathbf{W} + \sigma^2 \mathbf{I}$ (note that here the bases $\mathbf{W}$ are not required to be orthonormal). Using a Maximum Likelihood (ML) approach the parameters $\theta$ are found by solving

\begin{equation}
	\begin{aligned}
		\theta_o & = \underset{\theta}{\mathrm{arg\,min\;}} \ln \prod_{i=1}^n p(\mathbf{x}_i|\theta)
		\\
		& = \underset{\theta}{\mathrm{arg\,min\;}} \ln \prod_{i=1}^n \int_{\mathbf{y}} p(\mathbf{x}_i|\mathbf{y},\theta) p(\mathbf{y}) d\mathbf{y}.
	\end{aligned}
\end{equation}

with the optimal $\mathbf{W}$, $\bar{\mathbf{x}}$ and $\sigma^2$ given by

\begin{equation}
	\begin{aligned}
		\mathbf{W} & = \mathbf{U} (\mathbf{\Sigma} - \sigma^2 \mathbf{I})^{1/2} \mathbf{R} 
		\\
		\bar{\mathbf{x}} & = \frac{1}{N} \sum_{i=1}^N \mathbf{x}_i
		\\
		\mathbf{\sigma}^2 & = \frac{1}{N-M} \sum_{i=M+1}^N \lambda_i
	\end{aligned}
\end{equation}

where $\mathbf{R}$ is an arbitrary $M \times M$ orthogonal matrix.

An alternative to the ML approach is to use an Expectation Maximization (EM) procedure where the first and second order moments of the latent space ($\mathbf{E}[\mathbf{y}]$ and $\mathbf{E}[\mathbf{y} \mathbf{y}^T]$) are also found.
The EM solutions for the parameters can be found in \cite{bishop2006pattern, tipping1999probabilistic}. Several variations of probabilistic PCA have been proposed, e.g. by incorporating sparseness and nonnegative constraints \cite{guan2009sparse} or changing the Gaussian models for others (such as the Student-t model of \cite{khan2004robust}).
